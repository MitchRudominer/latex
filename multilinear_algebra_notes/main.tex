\documentclass[oneside,12pt]{amsart}

\usepackage{amsmath,amssymb,latexsym,eucal,amsthm}

\input{mathdefs}
\input{theoremstyles}

\pagestyle{plain}

\begin{document}

\title{Multilinear Algebra Notes}
\author{Mitch Rudominer}

\maketitle

\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Covariant and Contravariant Tensors}

Let $V$ and $W$ be finite-dimensional vector spaces, 
$\dim(V)=n$, $\dim(W) = m$. Let $\pi:V\map W$ be linear.

Let $\cB = \singleton{v_1,\cdots,v_n}$ be a basis for $V$ and
$\cC=\singleton{w_1,\cdots,w_m}$ a basis for $W$.

\begin{definition}
$V^*$, $W^*$ are the dual-spaces of $V$ and $W$ respectively.
$\pi^*:W^*\map V^*$ is the \emph{adjoint} of $\pi$ defined by
$\pi^*(\varphi) \cdot v = \varphi \cdot \pi(v)$.
\end{definition}

Note that above and going forward, if
$\varphi$ is a covector and $v$ is a vector we write $\varphi(v)$
as $\varphi \cdot v$.

\begin{definition}
 For any matrix $A$ we write $A=(a^i_j)$ to mean that $a^i_j$ is the
 element of $a$ in row $i$, column $j$. 
\end{definition}


We will use \emph{index notation} from physics in which an expression
like $a^i_j b^j$ means $\sum_j(a^i_j b^j)$.

The letter $\delta$ with two indices, upper or lower, will always mean the Kronecker delta. So for example $\delta^i_j$ equals 1 if $i=j$ and equals 0 if $i\neq j$. 

\begin{definition}
$M^{\pi}_{\cB,\cC}$ is the $n\times m$ matrix of $\pi$ in
the two bases:


 Letting $M^{\pi}_{\cB,\cC}=A=(a^i_j)$,
$a^i_j$ is defined by
$$\pi(c^jv_j) = a^i_j c^j w_i$$
for $1\leq i \leq n$, $1\leq j \leq m$.
\end{definition}

Let
$\hat{\cB} = \singleton{v^1,\cdots,v^n}$ and
$\hat{\cC} = \singleton{w^1,\cdots,w^m}$ be the corresponding
dual bases for $V^*$ and $W^*$ respectively. 
So $v^i \cdot v_j = \delta^i_j$ and $w^i \cdot w_j = \delta^i_j$.

\begin{lemma}
Let $A = M^{\pi}_{\cB,\cC} =(a^i_j)$.

Then $M^{\pi^*}_{\hat{\cC},\hat{\cB}}  = \transpose{A}$.

But we must be careful with the notation here!

Because we write vectors as column vectors and multiply them by matrices on the left
and we write co-vectors as row-vectors and multiply them by matrices on the right, it
is actually $A$ itself and not $\transpose{A}$ that we will use to represent
$M^{\pi^*}_{\hat{\cC},\hat{\cB}}$.


In index notation, we write the components of vectors using upper indices and the components
of covectors using lower indices. So when representing $\pi$, $A$ acts as
$c^j \to a^i_jc^j$ but when representing $\pi^*$, $A$ acts as $d_k \to a^k_j d_k$.
\end{lemma}
\begin{proof}
Let $v=c^j v_j$ and let $\varphi = d_k w^k$. Then $\pi^*(\varphi)\cdot v = \varphi \cdot \pi(v)
= d_k w^k \cdot a^i_j c^j w_i = d_k a^k_j c ^ j$. Therefore $\pi^*(\varphi) = d_k a^k_j v^j$.
\end{proof}

\begin{corollary}
Suppose $n=m$ and $\pi$ is a linear isomorphism. Then
$(\pi^*)^{-1}:V^* \map W^*$ is defined
and letting $A = M^{\pi}_{\cB,\cC}$ we have that
$M^{(\pi^*)^{-1}}_{\cB,\cC} = (\transpose{A})^{-1}$.

Again, because we write covectors as row vectors (equivalently we write the components of covectors using lower indices), it is actually $A^{-1}$ we will use to represent $(\pi^*)^{-1}.$

\end{corollary}

From now on assume $n=m$ and $\pi$ is a linear isomorphism. 

We will sometimes use the following notation:
If $A =(a^i_j)$ we will write $(\atwiddle^i_j)$ to implicitly
mean $A^{-1} = (\atwiddle^i_j)$.

Thus we have $a^i_j \atwiddle^j_k = \delta^i_k$.

So the above corollary can be stated like this:

If $\pi(c^j v_j) = a^i_j c^j w_i$, then
$(\pi^*)^{-1} (d_i v^i) = \atwiddle^i_j d_i w^j$.

\begin{proof}
Let $\varphi = d_i v^i$ and let $v=c^k v_k$.
Then 

$$\atwiddle^i_j d_i w^j \cdot a^l_k c^k w_l = 
\atwiddle^i_j d_i  a^j_k c^k =
\delta^i_k d_i c^k =
d_i c^i = \varphi \cdot v = (\pi^*)^{-1}(\varphi) \cdot \pi(v)$$

\end{proof}

\begin{corollary}
Suppose that not only is $\pi$ a linear isomorphism but that it preserves
the inner products induced by the two bases.

Letting $A = M^{\pi}_{\cB,\cC}$ we have that
$M^{(\pi^*)^{-1}}_{\cB,\cC} = A$.
\end{corollary}
\begin{proof}
$\pi$ preserves the inner product iff $\transpose{A} = A^{-1}$.
\end{proof}

\begin{note}
So the dual map in the dual bases has the same matrix as the
regular map in the regular bases.
Awkwardly, because we represent covectors using row vectors we would
need to use $\transpose{A}$ instead of $A$ to represent the dual map.

Another way to look at it is this: Let $v,w\in\R^n$. Then
$$\transpose{(Av)}Aw = \transpose{v}\transpose{A}Aw = \transpose{v}w$$
since $\transpose{A}A = I$.
\end{note}


\bibliographystyle{amsalpha}

\bibliography{math}

\end{document}
