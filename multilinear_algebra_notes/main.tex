\documentclass[oneside,12pt]{amsart}

\usepackage{amsmath,amssymb,latexsym,eucal,amsthm}

\input{mathdefs}
\input{theoremstyles}

\pagestyle{plain}

\begin{document}

\title{Multilinear Algebra Notes}
\author{Mitch Rudominer}

\maketitle

\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Covariant and Contravariant Tensors}

Let $V$ and $W$ be finite-dimensional vector spaces, 
$\dim(V)=n$, $\dim(W) = m$. Let $\pi:V\map W$ be linear.

Let $\cB = \singleton{v_1,\cdots,v_n}$ be a basis for $V$ and
$\cC=\singleton{w_1,\cdots,w_m}$ a basis for $W$.

We will use \emph{index notation} from physics in which an expression
like $a^i_j b^j$ means either the $m$-tuple 
$\sequence{\sum_{j=1}^n(a^i_j b^j)}{i=1\cdots m}$
or the number $\sum_{j=1}^n(a^i_j b^j)$ depending on the context. 
And $n$ and $m$ are also derived from the context.

The letter $\delta$ with two indices, upper or lower, will always mean the Kronecker delta. So for example $\delta^i_j$ equals 1 if $i=j$ and equals 0 if $i\neq j$. 
As above ``$\delta^i_j$" might refer to a single number or a 2-dimensional array
of numbers indexed by $i$ an $j$, depending on the context. In the case that
$\delta^i_j$ refers to a square matrix, then, it is the identity matrix.


\begin{definition}
 For any matrix $A$ we write $A=(a^i_j)$ to mean that $a^i_j$ is the
 element of $a$ in row $i$, column $j$. 
\end{definition}


\begin{definition}
$M^{\pi}_{\cB,\cC}$ is the $m\times n$ matrix of $\pi$ 
relative to the two bases:

 Letting $M^{\pi}_{\cB,\cC}=A=(a^i_j)$,
$a^i_j$ is defined by
$$\pi(c^jv_j) = a^i_j c^j w_i$$
for $1\leq i \leq m$, $1\leq j \leq n$.

In particular we have
$$\pi(v_j) = a^i_j  w_i$$
for $1\leq i \leq m$, $1\leq j \leq n$,
so that $a^i_j$ is the $i$-th component in the $\cC$-representation of the
image under $\pi$ of $v_j$.
\end{definition}

\begin{definition}
$V^*$, $W^*$ are the dual-spaces of $V$ and $W$ respectively.
$\pi^*:W^*\map V^*$ is the \emph{adjoint} of $\pi$ defined by
$\pi^*(\varphi) \cdot v = \varphi \cdot \pi(v)$.
\end{definition}

Note that above and going forward, if
$\varphi$ is a covector and $v$ is a vector we write $\varphi(v)$
as $\varphi \cdot v$.

Let
$\hat{\cB} = \singleton{v^1,\cdots,v^n}$ and
$\hat{\cC} = \singleton{w^1,\cdots,w^m}$ be the corresponding
dual bases for $V^*$ and $W^*$ respectively. 
So $v^i \cdot v_j = \delta^i_j$ and $w^i \cdot w_j = \delta^i_j$.

\begin{lemma}
Let $A = M^{\pi}_{\cB,\cC} =(a^i_j)$.

Then $M^{\pi^*}_{\hat{\cC},\hat{\cB}}  = \transpose{A}$.

But we must be careful with the notation here!

Because we write vectors as column vectors and multiply them by matrices on the left
and we write co-vectors as row-vectors and multiply them by matrices on the right, it
is actually $A$ itself and not $\transpose{A}$ that we will use to represent
$M^{\pi^*}_{\hat{\cC},\hat{\cB}}$.


In index notation, we write the components of vectors using upper indices and the components
of covectors using lower indices. So when representing $\pi$, $A$ acts as
$c^j \to a^i_jc^j$ but when representing $\pi^*$, $A$ acts as $d_k \to a^k_j d_k$.
\end{lemma}
\begin{proof}
Let $v=c^j v_j$ and let $\varphi = d_k w^k$. Then $\pi^*(\varphi)\cdot v = \varphi \cdot \pi(v)
= d_k w^k \cdot a^i_j c^j w_i = d_k a^k_j c ^ j$. Therefore $\pi^*(\varphi) = d_k a^k_j v^j$.
\end{proof}

\begin{remarks}
The above results are summarized here:
\begin{enumerate}
\item $v_j \mapsto a^i_j w_i$ (single vector) defines $\pi$ on  $\cB$ and $\cC$.
\item $c^j \mapsto a^i_j c ^j$ ($n$-tuple of numbers) ($\cB$ - $\cC$)-matrix of $\pi$.
\item $w^i \mapsto a^i_j v^j$ (single covector) defines $\pi^*$ on $\hat{\cC}$ and $\hat{\cB}$.
\item $d_i \mapsto a^i_j d_i$ ($m$-tuple of numbers) ($\hat{\cC}$ - $\hat{\cB}$)-matrix of $\pi^*$.
\end{enumerate}
\end{remarks}

\begin{corollary}
Suppose $n=m$ and $\pi$ is a linear isomorphism. Then
$(\pi^*)^{-1}:V^* \map W^*$ is defined
and letting $A = M^{\pi}_{\cB,\cC}$ we have that
$M^{(\pi^*)^{-1}}_{\cB,\cC} = (\transpose{A})^{-1}$.

Again, because we write covectors as row vectors (equivalently we write the components of covectors using lower indices), it is actually $A^{-1}$ we will use to represent $(\pi^*)^{-1}.$

\end{corollary}

From now on assume $n=m$ and $\pi$ is a linear isomorphism. 

We will sometimes use the following notation:
If $A =(a^i_j)$ we will write $(\atwiddle^i_j)$ to implicitly
mean $A^{-1} = (\atwiddle^i_j)$.

Thus we have $a^i_j \atwiddle^j_k = \delta^i_k$.

So the above corollary can be stated like this:

If $\pi(c^j v_j) = a^i_j c^j w_i$, then
$(\pi^*)^{-1} (d_i v^i) = \atwiddle^i_j d_i w^j$.


\begin{corollary}
Suppose that not only is $\pi$ a linear isomorphism but that it preserves
the inner products induced by the two bases.

Letting $A = M^{\pi}_{\cB,\cC}$ we have that
$M^{(\pi^*)^{-1}}_{\cB,\cC} = A$.
\end{corollary}
\begin{proof}
$\pi$ preserves the inner product iff $\transpose{A} = A^{-1}$.
\end{proof}

\begin{note}
So the dual map in the dual bases has the same matrix as the
regular map in the regular bases.
Awkwardly, because we represent covectors using row vectors we would
need to use $\transpose{A}$ instead of $A$ to represent the dual map.

Another way to look at it is this: Let $v,w\in\R^n$. Then
$$\transpose{(Av)}Aw = \transpose{v}\transpose{A}Aw = \transpose{v}w$$
since $\transpose{A}A = I$.
\end{note}


\bibliographystyle{amsalpha}

\bibliography{math}

\end{document}
